---
title: "Introduction to multi-objective reinforcement learning"
subtitle: ""
date: 20-10-2024
author: Santeri Heiskanen
institute: Aalto University
bibliography: citations.bib
format:
  revealjs: 
    slide-number: true
    theme: [default, theme.scss]
    chalkboard: 
      buttons: false
    preview-links: auto
    smaller: true
resources:
  - demo.pdf
---

## Multi-objective reinforcement learning in a nutshell {.smaller}

:::: {.columns}

::: {.column width="50%" .incremental}

- Multi-objective reinforcement learning (MORL) is an extension to single-objective
  reinforcement learning (SORL), where one considers multiple, *conflicting*
  objectives simultaneously.
- A policy $\pi$ is considered [Pareto optimal]{.alert} if it
  performs better than the other policies in at least one objective, and 
  is (at least) equally good in the other objectives.
- Goal of MORL is (often) to find a set of Pareto-optimal policies,
  called [Pareto front]{.alert}^[The exact goal is dependent on multiple factors,
  including the type of utility function.]
:::

::: {.column width="50%"}
![Conceptual Pareto front for two objectives. Each point in the
plot represents a value of policy $\pi$ with given preference $\mathbf{\omega}$ ](./figures/conceptual-pareto-front.svg){#fig-conceptual-pf width="500"
height="250"}
:::

::: {.notes}
- MORL consider multiple CONFLICTING OBJECTIVES.
- Intuitively, one tries to find an optimal trade-off between objectives.
- The final goal is to have a set of optimal trade-offs for different 
  prefernces.
:::

:::: 

## Why consider multiple objectives? {.center}

::: {.incremental}
1. Move responsibility of choosing the desired trade-off between the objectives
   from engineer to end-user.
2. The desired trade-off can be determined [after]{.alert} the optimization is
   done and the results can be observed.
3. There might uncertainty in user preferences between objectives.

:::

::: {.notes}
- Considering multiple objectives will complicate problem, so why bother?
- Some researchers argue that one objective is all you need (Barton).
- There is a complicated taxonomy available, yet, we will consider only few
  cases here.
- Instead of the engineering designing the results in ad-hoc way, the user 
  can deside the desired trade-off
- It is not always clear how certain trade-offs affect the found policy. with
  MORL one can first create the set of solutions, and then choose the desired 
  trade-off after observing the options.
- It is also possible that one does not know the preferences before hand / they 
  might change during the process.
- E.g. design specifications of mobile device change so that a better energy
  efficiency is required.
:::

## Example: Multi-objective Halfcheetah {.center}
:::: {.columns}

::: {.column width="33%"}
{{< video videos/halfcheetah_pref_0_overlay.mp4 width="250%">}}

:::

::: {.column width="33%"}
{{< video videos/halfcheetah_pref_3_overlay.mp4 width="250%">}}
:::

::: {.column width="33%"}
{{< video videos/halfcheetah_pref_5_overlay.mp4 width="250%">}}
:::


::::

- Two objectives: Energy consumption and running speed.
- All behaviours are generated by a [single policy]{.alert} that is conditioned
  on the preferences between the objectives.

::: {.notes}

- Practical example: Halfcheetah with two conflicting objectives.
- A single policy is used to generate different behaviours based on the desired
  trade-off between the objectives.
- The "optimal" behaviours clearly differ quite significantly. How to
  generalize? This is a common issue in the MORL context.
:::


## Multi-objective MDPs and value-functions {.smaller}

::: {.incremental}
- Formally, the multi-objective sequential decision making problems are
  presented as a *multi-objective* Markov decision process *MOMDP*: $\langle S,
  A, T, \gamma, \mu, \mathbf{R}\rangle$.
- The major difference to the traditional MDP's is the inclusion of
  vector valued reward function $\mathbf{R}:
  S \times A \times S \rightarrow \mathbb{R}^d$, where $d \geq 2$ is the number
  of objectives.

- [Vector-valued]{.alert} value-function is defined as in the single-objective case:
  $$\mathbf{V}^{\pi} = \mathbb{E}\left[\sum_{i=0}^\infty \gamma^i
  \mathbf{r}_{i+1}\,\vert\,\pi, \mu\right]$$

- Consider two policies $\pi$ and $\pi'$ with two objectives $i$ and $j$:
  - It is possible to find situation such that $V_i^\pi > V_i^{\pi'}$  while 
    $V_j^\pi < V_j^{\pi'}$
  - Value-function defines only *partial* ordering over the value-space.

:::

::: {.notes}
 - NOTE: This is very heavy, yet important slide on mathematics.
 - Main difference between MDP's and MOMDP's is the vector-valued reward
   function.
 - Note: usually $d$ is 2 or 3. While theory covered here does not limit $d$,
   methods that work for low d usually do not work well with high d.
 - The value-function can be defined as with the MDPs bu just swapping
   scalarized reward to the vector-valued reward.
 - Issue: In MPDs, one could find the optimal policy by finding a the optimal
   value-function. In MORL, the same does not hold.
 - One needs information about the user prefernces over the objectives.
:::

## Utility functions {.center}

::: {.incremental}
- Utility function (or scalarization function) is a mapping that encodes the user 
  [preferences]{.alert} over the objectives:
  $$
  u: \mathbb{R}^d \rightarrow \mathbb{R}
  $$
- In practice, the utility functions are parameterized by user preferences
  $\mathbf{w}$ over the objectives.
- The vector-valued value-function can be converted back to
  scalar-valued function using the utility function 
  $$
  V_u^\pi = u(\mathbf{V}^\pi)
  $$
- The scalarized value-function defines a total ordering of the value-space
  for a *given* preference.


:::

::: {.notes}

- To fix the issue with vector-valued value-function, we need to consider the 
  user preferences over the objectives.
- Intuition: Utility functions encode the user preferences, and reduce the 
  MO optimization back to scalar valued case.
- The user preferences is usually presented as a weight vector, where each 
  objective presents the relative performance of the objectives. Easy to
  achieve by normalizing the vector.
- Scalarized utility functions define total ordering of policies for the given
  preferences. *EMPHASIZE THIS*
:::


## Types of utility functions {.center}

::: {.incremental}
- Intuitively, the scalarized value-function should *increase*, if the value
  of one of the objectives increases, while values of other objectives stay the same.
  - i.e. $u([0.5, 0.5]) > u([0.5, 0.1])$.

- [Monotonically increasing]{.alert} utility functions fulfill this
  requirement.
  - This is a minimal assumption about the form of the utility function. 
  - Crucially, this set contains non-linear utility functions, such as Tchebycheff
    utility function [@perny_finding_2010].

- [Linear]{.alert} utility functions are commonly used in practice
  - $V_{\mathbf{w}}^\pi = \mathbf{w}^T \mathbf{V}^\pi$
  - In addition of having certain crucial theoretical properties, they are easy 
    to interpret.
:::

::: {.notes}

- There are two common classes of utility functions that one can consider.
- Minimal assumption: Increase in one value also increases the scalarized
  value. Restriction comes from intuitive idea of a reward.
- Monotonically increasing functions are the widest class of functions that 
  fulfill this criteria.
- Other, more commonly researched set of functions are linear utility
  functions.
- Are easy to interpret, and hold certain assumptions as will be seen.

:::


## What to optimize for? {.center}

::: {.incremental}
- In SORL, the goal is the optimize the [expected cumulative return]{.alert}.
- MORL, there are two optimality criteria:
  - *Scalarized expected return (SER)*
    $$
    V_u^\pi = u\Biggl(\mathbb{E}\left[\sum_{i=0}^\infty \gamma^t
    \mathbf{r}_i\,\bigg\vert\,\pi, s_0\right] \Biggl)
    $$

  - *Expected scalarized return (ESR)*
    $$
    V_u^\pi =  \mathbb{E}\left[u\left(\sum_{i=0}^\infty \gamma^t \mathbf{r}_i\right)\,\bigg\vert\,\pi,
    s_0\right] 
    $$
- If the utility function is [linear]{.alert}, the criteria produce same
  policies.
:::

::: {.notes}
- In SORL case, the goal was to optimize the expected cumulative return
- In MORL, there are two ways of defining the optimality criterion.
- SER: User utility calculated over many runs -> Best when the utility can be 
  collected over multiple runs
- ESR: User utility calculated from a single run -> Best when one cares about
  the utility during single run. For example, medical treatments.
:::

## Bellman equation with ESR {.center}

::: {.incremental}

- Recall Bellman equation:
  $$
  \mathbf{V}^{\pi}(s) = \mathbf{R}_t + \gamma \sum_{s'} P\big(s'\,\vert\,s, \pi(s)\big)\mathbf{V}^\pi(s')
  $$

- Consider what happens with the ESR optimality criterion with non-linear utility function.
- 
  $$
  \mathbb{E}\left[u\bigg(\mathbf{R}_t + \sum_{i=t}^\infty \gamma^i \mathbf{r}_i\bigg)\,\bigg\vert\,\pi, s_t \right] \neq u(\mathbf{R}_t) + \mathbb{E}\left[ u\bigg(\sum_{i=t}^\infty \gamma^i \mathbf{r}_i\bigg)\,\bigg\vert\, \pi, s_t\right]
  $$
- Implication: most existing methods considering MDPs cannot be used with 
  ESR optimality criterion and non-linear utility functions.
- One needs to take into account the previously accumulated rewards.

:::

::: {.notes}
- Bellman equation: Value function is the sum of immediate return and the 
  expected return from this state onwards.
- The Bellman equation assumes the additivity of the expectation.
- Most of the solulations considering MDP's use this property.
- If the utility function is non-linear, the expectation cannot be splitted
  into immediate reward and future rewards.
- Implication:  One needs to take into account the previously accumulated
  rewards when optimizing for the policy at state $s_t$.
- This has limited the use of ESR, since a new methods must be developed to 
  solve problems with criterion.
:::



## How to solve a MORL problem {.center}

::: {.incremental}
- Previous slides: The agent optimizes the [scalarized expected return/expected
  scalarized return]{.alert}. Howver, our goal is to find a *set of solutions*.

- [@roijers_multi-objective_2017] identified two approaches for solving a MORL
  task.

- **Outer loop methods**: 

   Run single-objective algorithms with different user prefererences
     repeatetly.

   ```{.python}
   solutions = set()
   for w in W:
      pi_w = sorl(w) # Solve single-objective problem
      solutions.add(pi_w)
   ```
- **Inner loop methods**: Modify the underlying algorithm, and directly generate a solution set.
:::

::: {.notes}
- Consider only multi-solution MORL. In a single-objective case, the solutions
  are more simple.
- NOTE: The agent optimizes the scalarized reward, while we care about the 
  whole set. 
- Inner loop methods are simple. However, they have two common issues
  1. How to select prefernces? close preferences likely to produce same
     policies.
  2. How to share information between learned policies.
:::

## Are deterministic stationary policies enough?

:::: {.columns}

::: {.column width="55%" .incremental}
 - If utility function is linear, then deterministic stationary policies 
   can produce optimal solutions.
   [@roijers_survey_2013]^[This holds under the utility based viewpoint]
 - Deterministic policies from @fig-one-state-momdp have values: 
   $\mathbf{V}^{\pi_1} = \big(3 / (1 - \gamma), 0\big)$,
   $\mathbf{V}^{\pi_2} = \big(0, 3 / (1 - \gamma)\big)$,
   $\mathbf{V}^{\pi_3} = \big(1/(1 - \gamma), 1/(1-\gamma)\big)$
- Create a *mixture* policy that selects action $a_1$ with probability $p$,
  and action $a_2$ otherwise [@vamplew_constructing_2009]
   - $\mathbf{V}^{\pi_m} = \big(3p / (1 - \gamma), 3(1-p)/(1 - \gamma)\big)$

:::

::: {.column width="45%"}
   
![One state MOMDP with three actions. Example follows @roijers_survey_2013, adapted from @white_multi-objective_1982](./figures/one-state-momdp-v2.png){#fig-one-state-momdp height="300%"}

:::

::: {.incremental}
- To show that non-stationary policy can dominate a stationary policy, consider
  similar policy to above, which alternates between actions $a_1$ and $a_2$,
  starting from $a_1$ @white_multi-objective_1982.
:::

::: {.notes}
- NOTE: The slide is again quite dense. Take your time to explain the idea.
- For MPD's, there always exists a stationary deterministic optimal policy.
- Three deterministic policies, where one takes one of the actions
- By constructing a mixture policy, one can create a policy that dominates the 
  deterministic policy $pi^3$ when $\gamma > 0.5$
- It is not enough to consider stochastic policies. However, note that the 
  first result holds when considering linear utility function.
- NOTE: This same example can be used to shown that stationary policies 
  are not enough either.
:::

:::: 

## Value functions are convex

::: {.incremental}
- Can non-convex portions of the Pareto-front be recovered using the
  linear utility functions?
- [@lu_multi-objective_2023]: The value-functions are convex. Theoretically it 
  is possible to find the whole Pareto-front with linear utility function.
- Existing algorithms have two bottlenecks:
  1. Preference for deterministic policies.
  2. Numerical instability.
- These issues can be remedied by augmenting the reward-function with a strongly
  concave term^[Here $\mathcal{H}(q) = - \int q(a) \log q(a) da$ is the entropy
  operator. However, in theory any strongly concave function can be used.]:
  $$
  \pi(\cdot;\mathbf{w})
  = \underset{\pi'(\cdot;\mathbf{w})}{\mathrm{arg\;max}}\,
  \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t \big(\mathbf{w}^T \mathbf{R}(a_t,
  s_t) + \alpha \mathcal{H}(\pi'(s_t;\mathbf{w}))\big)\right]
  $$
- This is a multi-objective version of Soft-Actor Critic [@haarnoja_soft_2018].
:::



::: {.notes}
- As we have seen previously, the linear utility functions are convinient:
  1. One can use existing methods
  2. They are simple to understand and use.
- Issue: They have issues recovering non-convex portions of the Pareto-front
- It was not sure if it was even theoretically find the whole Pareto-front
  when using linear utility functions.
- Convex value-functions: One can theoretically find the PF by traversing over 
  all preferences. 
- NOTE: This justifies the use of the outer-loop methods!

:::


## Performance indicators

::: {.incremental}
- Two different view points: [axiomatic]{.alert} and [utility based]{.alert}
- Advantages and disadvantages of both cases.
- The disagreement of the agent optimizing the reward, while we care about the
  properties of the set of solutions.
- Examples of these indicators.

:::


## Open questions and challenges

## References
::: {#refs}

:::


## Fragments

Incremental text display and animation with fragments:

<br/>

::: {.fragment .fade-in}
Fade in
:::

::: {.fragment .fade-up}
Slide up while fading in
:::

::: {.fragment .fade-left}
Slide left while fading in
:::

::: {.fragment .fade-in-then-semi-out}
Fade in then semi out
:::

. . .

::: {.fragment .strike}
Strike
:::

::: {.fragment .highlight-red}
Highlight red
:::

::: footer
Learn more: [Fragments](https://quarto.org/docs/presentations/revealjs/advanced.html#fragments)
:::

## Slide Backgrounds {background="#43464B"}

Set the `background` attribute on a slide to change the background color (all CSS color formats are supported).

Different background transitions are available via the `background-transition` option.

::: footer
Learn more: [Slide Backgrounds](https://quarto.org/docs/presentations/revealjs/#color-backgrounds)
:::

## Auto-Animate {auto-animate="true" auto-animate-easing="ease-in-out"}

Automatically animate matching elements across slides with Auto-Animate.

::: r-hstack
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 200px; height: 150px; margin: 10px;"}
:::

::: {data-id="box2" auto-animate-delay="0.1" style="background: #3fb618; width: 200px; height: 150px; margin: 10px;"}
:::

::: {data-id="box3" auto-animate-delay="0.2" style="background: #e83e8c; width: 200px; height: 150px; margin: 10px;"}
:::
:::

::: footer
Learn more: [Auto-Animate](https://quarto.org/docs/presentations/revealjs/advanced.html#auto-animate)
:::

## Auto-Animate {auto-animate="true" auto-animate-easing="ease-in-out"}

Automatically animate matching elements across slides with Auto-Animate.

::: r-stack
::: {data-id="box1" style="background: #2780e3; width: 350px; height: 350px; border-radius: 200px;"}
:::

::: {data-id="box2" style="background: #3fb618; width: 250px; height: 250px; border-radius: 200px;"}
:::

::: {data-id="box3" style="background: #e83e8c; width: 150px; height: 150px; border-radius: 200px;"}
:::
:::

::: footer
Learn more: [Auto-Animate](https://quarto.org/docs/presentations/revealjs/advanced.html#auto-animate)
:::

## Slide Transitions {.smaller}

The next few slides will transition using the `slide` transition

| Transition | Description                                                            |
|------------|------------------------------------------------------------------------|
| `none`     | No transition (default, switch instantly)                              |
| `fade`     | Cross fade                                                             |
| `slide`    | Slide horizontally                                                     |
| `convex`   | Slide at a convex angle                                                |
| `concave`  | Slide at a concave angle                                               |
| `zoom`     | Scale the incoming slide so it grows in from the center of the screen. |

::: footer
Learn more: [Slide Transitions](https://quarto.org/docs/presentations/revealjs/advanced.html#slide-transitions)
:::

## Interactive Slides {.smaller transition="slide"}

Include Jupyter widgets and htmlwidgets in your presentations

Cool

::: footer
Learn more: [Jupyter widgets](https://quarto.org/docs/interactive/widgets/jupyter.html), [htmlwidgets](https://quarto.org/docs/interactive/widgets/htmlwidgets.html)
:::

## Preview Links

Navigate to hyperlinks without disrupting the flow of your presentation.

Use the `preview-links` option to open links in an iframe on top of your slides. Try clicking the link below for a demonstration:

::: {style="text-align: center; margin-top: 1em"}
[Matplotlib: Visualization with Python](https://matplotlib.org/){preview-link="true" style="text-align: center"}
:::

::: footer
Learn more: [Preview Links](https://quarto.org/docs/presentations/revealjs/presenting.html#preview-links)
:::


